{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Predicting Boston Housing Prices\n",
    "In this project, we will evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a *good fit* could then be used to make certain predictions about a home â€” in particular, its monetary value. This model would prove to be invaluable for someone like a real estate agent who could make use of such information on a daily basis.\n",
    "\n",
    "The dataset for this project originates from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Housing). The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts. For the purposes of this project, the following preprocessing steps have been made to the dataset:\n",
    "- 16 data points have an `'MEDV'` value of 50.0. These data points likely contain **missing or censored values** and have been removed.\n",
    "- 1 data point has an `'RM'` value of 8.78. This data point can be considered an **outlier** and has been removed.\n",
    "- The features `'RM'`, `'LSTAT'`, `'PTRATIO'`, and `'MEDV'` are essential. The remaining **non-relevant features** have been excluded.\n",
    "- The feature `'MEDV'` has been **multiplicatively scaled** to account for 35 years of market inflation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Run the code cell below to load the Boston housing dataset, along with a few of the necessary Python libraries required for this project. We will know the dataset loaded successfully if the size of the dataset is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston housing dataset has 489 data points with 4 variables each.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries necessary\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Import supplementary visualizations code visuals.py\n",
    "import visuals as vs\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Initialize Spark and SQL Contexts\n",
    "spark_context = SparkContext()\n",
    "sc = SQLContext(spark_context)\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "data = sc.read.csv('housing.csv', header=True, inferSchema=True)\n",
    "prices = data.select('MEDV')\n",
    "features = data.drop('MEDV')\n",
    "\n",
    "# Success\n",
    "print(\"Boston housing dataset has {} data points with {} variables each.\".format(data.count(), len(data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Exploration\n",
    "In this first section of this project, we will make a cursory investigation about the Boston housing data and provide our observations. Familiarizing yourself with the data through an explorative process is a fundamental practice to help better understand and justify the results.\n",
    "\n",
    "Since the main goal of this project is to construct a working model which has the capability of predicting the value of houses, we will need to separate the dataset into **features** and the **target variable**. The **features**, `'RM'`, `'LSTAT'`, and `'PTRATIO'`, give us quantitative information about each data point. The **target variable**, `'MEDV'`, will be the variable we seek to predict. These are stored in `features` and `prices`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Calculate Statistics\n",
    "We will calculate descriptive statistics about the Boston housing prices. These statistics will be extremely important later on to analyze various prediction results from the constructed model.\n",
    "\n",
    "In the code cell below, we have implemented the following:\n",
    "- Calculated the minimum, maximum, mean, median, and standard deviation of `'MEDV'`, which is stored in `prices`.\n",
    "  - Stored each calculation in their respective variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Boston housing dataset:\n",
      "\n",
      "Minimum price: $105,000.00\n",
      "Maximum price: $1,024,800.00\n",
      "Mean price: $454,342.94\n",
      "Median price $438,900.00\n",
      "Standard deviation of prices: $165,171.13\n"
     ]
    }
   ],
   "source": [
    "# Minimum price of the data\n",
    "minimum_price = prices.agg({'MEDV':'min'}).collect()[0]['min(MEDV)']\n",
    "\n",
    "# Maximum price of the data\n",
    "maximum_price = prices.agg({'MEDV':'max'}).collect()[0]['max(MEDV)']\n",
    "\n",
    "# Mean price of the data\n",
    "mean_price = prices.agg({'MEDV':'avg'}).collect()[0]['avg(MEDV)']\n",
    "\n",
    "# Median price of the data\n",
    "median_price = prices.approxQuantile('MEDV', [0.5], 0)[0]\n",
    "\n",
    "# Standard deviation of prices of the data\n",
    "std_price = prices.agg({'MEDV':'stddev_pop'}).collect()[0]['stddev_pop(MEDV)']\n",
    "\n",
    "# Show the calculated statistics\n",
    "print(\"Statistics for Boston housing dataset:\\n\")\n",
    "print(\"Minimum price: ${:,.2f}\".format(minimum_price))\n",
    "print(\"Maximum price: ${:,.2f}\".format(maximum_price))\n",
    "print(\"Mean price: ${:,.2f}\".format(mean_price))\n",
    "print(\"Median price ${:,.2f}\".format(median_price))\n",
    "print(\"Standard deviation of prices: ${:,.2f}\".format(std_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Feature Observation\n",
    "As a reminder, we are using three features from the Boston housing dataset: `'RM'`, `'LSTAT'`, and `'PTRATIO'`. For each data point (neighborhood):\n",
    "- `'RM'` is the average number of rooms among homes in the neighborhood.\n",
    "- `'LSTAT'` is the percentage of homeowners in the neighborhood considered \"lower class\" (working poor).\n",
    "- `'PTRATIO'` is the ratio of students to teachers in primary and secondary schools in the neighborhood.\n",
    "\n",
    "\n",
    "** Using your intuition, for each of the three features above, do you think that an increase in the value of that feature would lead to an **increase** in the value of `'MEDV'` or a **decrease** in the value of `'MEDV'`? Justify your answer for each.**\n",
    "\n",
    "**Hint:** This problem can be phrased using examples like below.  \n",
    "* Would you expect a home that has an `'RM'` value(number of rooms) of 6 be worth more or less than a home that has an `'RM'` value of 7?\n",
    "* Would you expect a neighborhood that has an `'LSTAT'` value(percent of lower class workers) of 15 have home prices be worth more or less than a neighborhood that has an `'LSTAT'` value of 20?\n",
    "* Would you expect a neighborhood that has an `'PTRATIO'` value(ratio of students to teachers) of 10 have home prices be worth more or less than a neighborhood that has an `'PTRATIO'` value of 15?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "\n",
    "**'RM':** More number of rooms mean high price, because the area of the plot on which the house is may be more than the one with lesser rooms. The designing and construction of the house would also have cost more, which justifies the high price.\n",
    "\n",
    "**'LSTAT':** The higher the percentage of lower class homeowners, the lesser the price of the house. This is because the prices of the neighbouring houses also have an impact on the price of the house based on demand and the class of the people trying to buy the house in that area.\n",
    "\n",
    "**'PTRATIO':** Lower PTRATIO could signify good education settings in the nearby schools, which increases the demand of such areas and the houses in these areas, which means an increase in the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Developing a Model\n",
    "In this second section of the project, we have developed the tools and techniques necessary for a model to make a prediction. Being able to make accurate evaluations of each model's performance through the use of these tools and techniques helps to greatly reinforce the confidence in our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Define a Performance Metric\n",
    "It is difficult to measure the quality of a given model without quantifying its performance over training and testing. This is typically done using some type of performance metric, whether it is through calculating some type of error, the goodness of fit, or some other useful measurement. For this project, we have calculated the [*coefficient of determination*](http://stattrek.com/statistics/dictionary.aspx?definition=coefficient_of_determination), R<sup>2</sup>, to quantify your model's performance. The coefficient of determination for a model is a useful statistic in regression analysis, as it often describes how \"good\" that model is at making predictions. \n",
    "\n",
    "The values for R<sup>2</sup> range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the **target variable**. A model with an R<sup>2</sup> of 0 is no better than a model that always predicts the *mean* of the target variable, whereas a model with an R<sup>2</sup> of 1 perfectly predicts the target variable. Any value between 0 and 1 indicates what percentage of the target variable, using this model, can be explained by the **features**. _A model can be given a negative R<sup>2</sup> as well, which indicates that the model is **arbitrarily worse** than one that always predicts the mean of the target variable._\n",
    "\n",
    "For the `performance_metric` function in the code cell below, we have implemented the following:\n",
    "- Used `r2` from `RegressionMetrics` in the `pyspark.mllib.evaluation` module to perform a performance calculation between `true_and_predicted`.\n",
    "- Assigned the performance score to the `score` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import 'RegressionMetrics'\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "\n",
    "def performance_metric(true_and_predicted):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    \n",
    "    # Calculate the performance score between 'true' and 'predictions'\n",
    "    score = RegressionMetrics(true_and_predicted.rdd.map(lambda row: (row.true, row.predictions))).r2\n",
    "    \n",
    "    # Return the score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - Goodness of Fit\n",
    "Assume that a dataset contains five data points and a model made the following predictions for the target variable:\n",
    "\n",
    "| True Value | Prediction |\n",
    "| :-------------: | :--------: |\n",
    "| 3.0 | 2.5 |\n",
    "| -0.5 | 0.0 |\n",
    "| 2.0 | 2.1 |\n",
    "| 7.0 | 7.8 |\n",
    "| 4.2 | 5.3 |\n",
    "\n",
    "Run the code cell below to use the `performance_metric` function and calculate this model's coefficient of determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has a coefficient of determination, R^2, of 0.936.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the performance of this model\n",
    "sample_vals = list(zip([3.0, -0.5, 2.0, 7.0, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3]))\n",
    "sample_data = sc.createDataFrame(sample_vals, ['true_vals', 'predictions'])\n",
    "score = performance_metric(sample_data.selectExpr('true_vals as true', 'predictions'))\n",
    "print(\"Model has a coefficient of determination, R^2, of {:.3f}.\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Would you consider this model to have successfully captured the variation of the target variable? \n",
    "* Why or why not?\n",
    "\n",
    "** Hint: **  The R2 score is the proportion of the variance in the dependent variable that is predictable from the independent variable. In other words:\n",
    "* R2 score of 0 means that the dependent variable cannot be predicted from the independent variable.\n",
    "* R2 score of 1 means the dependent variable can be predicted from the independent variable.\n",
    "* R2 score between 0 and 1 indicates the extent to which the dependent variable is predictable. An \n",
    "* R2 score of 0.40 means that 40 percent of the variance in Y is predictable from X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "A high R2 score of 0.936 for the model indicates that it has successfully captured 93.6% of the variation of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Shuffle and Split Data\n",
    "Our next implementation requires that we take the Boston housing dataset and split the data into training and testing subsets. Typically, the data is also shuffled into a random order when creating the training and testing subsets to remove any bias in the ordering of the dataset.\n",
    "\n",
    "For the code cell below, we have implemented the following:\n",
    "- Used `randomSplit` to shuffle and split the `features` and `prices` data into training and testing sets.\n",
    "  - Split the data into 80% training and 20% testing.\n",
    "  - Set the `seed` to a value of our choice. This ensures results are consistent.\n",
    "- Assigned the train and testing splits to `X_train`, `X_test`, `y_train`, and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing split was successful.\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and split the data into training and testing subsets\n",
    "X_train, X_test = data.randomSplit([0.8, 0.2], 7777777)\n",
    "y_train = X_train.select('MEDV')\n",
    "y_test = X_test.select('MEDV')\n",
    "X_train = X_train.drop('MEDV')\n",
    "X_test = X_test.drop('MEDV')\n",
    "\n",
    "# Success\n",
    "print(\"Training and testing split was successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - Training and Testing\n",
    "\n",
    "* What is the benefit to splitting a dataset into some ratio of training and testing subsets for a learning algorithm?\n",
    "\n",
    "**Hint:** Think about how overfitting or underfitting is contingent upon how splits on data is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "\n",
    "The benefit of splitting a dataset into some ratio is to train a model on the larger portion and to use the smaller portion as unseen data. This said portion of unseen data is used for testing the models performance. Using all of the data for training could result in models which might overfit or underfit on the unseen data. Therefore, we ensure that the model does good using a portion of the data we have for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
